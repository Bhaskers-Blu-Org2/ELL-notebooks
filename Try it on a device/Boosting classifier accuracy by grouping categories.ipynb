{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Boosting classifier accuracy by grouping categories\n",
    "\n",
    "In this tutorial, we will split the 1000 image-categories, which our model was trained to classify, into three disjoint sets: *dogs*, *cats*, and *other* (anything that isn't a dog or a cat). We will demonstrate how a classifier with low accuracy on the original 1000-class problem can have a sufficiently high accuracy on the simpler 3-class problem. We will write a Python script that reads images from the camera, barks when it sees a dog, and meows when it sees a cat.\n",
    "\n",
    "[![screenshot](https://microsoft.github.io/ELL/tutorials/Boosting-classifier-accuracy-by-grouping-categories/thumbnail.png)](https://youtu.be/SOmV8tzg_DU)\n",
    "\n",
    "#### Materials\n",
    "\n",
    "* Laptop or desktop computer\n",
    "* Raspberry Pi\n",
    "* Headphones or speakers for your Raspberry Pi\n",
    "* Raspberry Pi camera or USB webcam\n",
    "* *optional* - Active cooling attachment (see our [tutorial on cooling your Pi](https://microsoft.github.io/ELL/tutorials/Active-cooling-your-Raspberry-Pi-3/))\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "* Install [Jupyter](http://jupyter.readthedocs.io/en/latest/install.html) on your computer\n",
    "* Follow the instructions for [setting up your Raspberry Pi](https://microsoft.github.io/ELL/tutorials/Setting-up-your-Raspberry-Pi).\n",
    "* Complete the basic tutorial, [Getting started with image classification on Raspberry Pi](https://notebooks.azure.com/microsoft-ell/libraries/tutorials/html/Getting%20started%20with%20image%20classification%20on%20the%20Raspberry%20Pi%20%28Part%201%29.ipynb), to learn how to use an ELL model from the Gallery.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The pre-trained models in the [ELL gallery](https://microsoft.github.io/ELL/gallery/) are trained to identify 1000 different image categories (see the category names [here](https://github.com/Microsoft/ELL-models/raw/master/models/ILSVRC2012/categories.txt)). Often times, we are only interested in a subset of these categories and we don't require the fine-grained categorization that the model was trained to provide. For example, we may want to classify images of dogs versus images of cats, whereas the model is actually trained to distinguish between 6 different varieties of cat and 106 different varieties of dog.\n",
    "\n",
    "The dogs versus cats classification problem is easier than the original 1000 class problem, so a model that isn't very accurate on the original problem may be perfectly adequate on the simpler problem. Specifically, we will use a model that has an error rate of 64% on the 1000-class problem, but only 5% on the 3-class problem. We will build an application that grabs a frame from a camera, plays a barking sound when it recognizes one of the dog varieties, and plays a meow sound when it recognizes one of the cat varieties.\n",
    "\n",
    "As a pre-step, we need to install `ell` in the Azure virtual machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 'conda-forge' already in 'channels' list, moving to the top\n",
      "Warning: 'microsoft-ell' already in 'channels' list, moving to the top\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackageNotFoundError: Packages missing in current channels:\n",
      "            \n",
      "  - ell\n",
      "\n",
      "We have searched for the packages in the following channels:\n",
      "            \n",
      "  - https://conda.anaconda.org/microsoft-ell/win-64\n",
      "  - https://conda.anaconda.org/microsoft-ell/noarch\n",
      "  - https://conda.anaconda.org/conda-forge/win-64\n",
      "  - https://conda.anaconda.org/conda-forge/noarch\n",
      "  - https://repo.continuum.io/pkgs/main/win-64\n",
      "  - https://repo.continuum.io/pkgs/main/noarch\n",
      "  - https://repo.continuum.io/pkgs/free/win-64\n",
      "  - https://repo.continuum.io/pkgs/free/noarch\n",
      "  - https://repo.continuum.io/pkgs/r/win-64\n",
      "  - https://repo.continuum.io/pkgs/r/noarch\n",
      "  - https://repo.continuum.io/pkgs/pro/win-64\n",
      "  - https://repo.continuum.io/pkgs/pro/noarch\n",
      "  - https://repo.continuum.io/pkgs/msys2/win-64\n",
      "  - https://repo.continuum.io/pkgs/msys2/noarch\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda config --prepend channels conda-forge --prepend channels microsoft-ell\n",
    "!conda install -y ell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Deploy a pre-trained model on a Raspberry Pi\n",
    "\n",
    "Start by repeating the steps of the basic tutorial, [Getting Started with Image Classification on Raspberry Pi](https://notebooks.azure.com/microsoft-ell/libraries/tutorials/html/Getting%20started%20with%20image%20classification%20on%20the%20Raspberry%20Pi%20%28Part%201%29.ipynb). This time, specify the Gallery model by name, specifically one that is faster and less accurate. As before, download the model and compile it for the Raspberry Pi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling...\n",
      "compiled model up to date\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'boosting\\\\pi3\\\\CMakeLists.txt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ell.pretrained_model import PretrainedModel\n",
    "import ell.platform\n",
    "\n",
    "pretrained_model = PretrainedModel('d_I160x160x3NCMNCMNBMNBMNBMNBMNC1A')\n",
    "pretrained_model.download('boosting', rename='model')\n",
    "pretrained_model.compile(ell.platform.PI3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in all 1000 labels from the label file. All the pet labels happen to be at the beginning of the list, in scattered locations. To keep things manageable, we consider only the first 240 labels, which includes all the dogs and cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "categories = [line.strip('\\n') for line in open('boosting/categories.txt', 'r').readlines()]\n",
    "dogs = categories[151:270]\n",
    "cats = categories[281:294]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Write a script \n",
    "\n",
    "We will write a Python script that invokes the model on a Raspberry Pi, groups the categories as described above, and takes action if a dog or cat is recognized. ** As with the previous tutorial, change the `ip` and `user` arguments to your Raspberry Pi's IP address and your user name before running the code in the cell below. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57b95d0ab4d4210b58c73b83effb552"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION\n",
      "? file, file cabinet, filing cabinet\n"
     ]
    }
   ],
   "source": [
    "%%rpi --user=pi --ip=157.54.152.78 --rpipath=/home/pi/compare --model=pretrained_model\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import tutorialHelpers as helpers\n",
    "import subprocess\n",
    "if (os.name == \"nt\"):\n",
    "    import winsound\n",
    "\n",
    "# import the ELL model's Python module\n",
    "import model\n",
    "\n",
    "# Function to return an image from our camera using OpenCV\n",
    "def get_image_from_camera(camera):\n",
    "    if camera is not None:\n",
    "        # if predictor is too slow frames get buffered, this is designed to flush that buffer\n",
    "        ret, frame = camera.read()\n",
    "        if (not ret):\n",
    "            raise Exception('your capture device is not returning images')\n",
    "        return frame\n",
    "    return None\n",
    "\n",
    "# Return an array of strings corresponding to the model's recognized categories or classes.\n",
    "# The order of the strings in this file are expected to match the order of the\n",
    "# model's output predictions.\n",
    "def get_categories_from_file(fileName):\n",
    "    labels = []\n",
    "    with open(fileName) as f:\n",
    "        labels = f.read().splitlines()\n",
    "    return labels\n",
    "\n",
    "# Returns True if an element of the comma separated label `a` is an element of the comma separated label `b`\n",
    "def labels_match(a, b):\n",
    "    x = [s.strip().lower() for s in a.split(',')]\n",
    "    y = [s.strip().lower() for s in b.split(',')]\n",
    "    for w in x:\n",
    "        if (w in y):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Returns True if the label is in the set of labels\n",
    "def label_in_set(label, label_set):\n",
    "    for x in label_set:\n",
    "        if labels_match(label, x):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Declare variables that define where to find the sounds files we will play\n",
    "script_path = os.path.dirname(os.path.abspath(__file__))\n",
    "woof_sound = os.path.join(script_path, \"woof.wav\")\n",
    "meow_sound = os.path.join(script_path, \"meow.wav\")\n",
    "\n",
    "# Helper function to play a sound\n",
    "def play(filename):\n",
    "    if (os.name == \"nt\"):\n",
    "        winsound.PlaySound(filename, winsound.SND_FILENAME | winsound.SND_ASYNC)\n",
    "    else:\n",
    "        command = [\"aplay\", filename]\n",
    "        proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=0, universal_newlines = True)\n",
    "        proc.wait()\n",
    "\n",
    "# Helper function to decide what action to take when we detect a group\n",
    "def take_action(group):\n",
    "    if group == \"Dog\":\n",
    "        # A prediction in the dog category group was detected, play a `woof` sound\n",
    "        play(woof_sound)\n",
    "    elif group == \"Cat\":\n",
    "        # A prediction in the cat category group was detected, play a `meow` sound\n",
    "        play(meow_sound)\n",
    "\n",
    "# Open the video camera. To use a different camera, change the camera index.\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "# Read the category labels\n",
    "categories = get_categories_from_file(\"categories.txt\")\n",
    "dogs = categories[151:270]\n",
    "cats = categories[281:294]\n",
    "\n",
    "# Get the model's input dimensions. We'll use this information later to resize images appropriately.\n",
    "inputShape = model.get_default_input_shape()\n",
    "\n",
    "# Create a vector to hold the model's output predictions\n",
    "outputShape = model.get_default_output_shape()\n",
    "predictions = model.FloatVector(outputShape.Size())\n",
    "\n",
    "headerText = \"\"\n",
    "\n",
    "# Get an image from the camera. If you'd like to use a different image, load the image from some other source.\n",
    "image = get_image_from_camera(camera)\n",
    "\n",
    "# Prepare the image to pass to the model. This helper:\n",
    "# - crops and resizes the image maintaining proper aspect ratio\n",
    "# - reorders the image channels if needed\n",
    "# - returns the data as a ravelled numpy array of floats so it can be handed to the model\n",
    "input = helpers.prepare_image_for_model(image, inputShape.columns, inputShape.rows)\n",
    "\n",
    "# Get the predicted classes using the model's predict function on the image input data. \n",
    "# The predictions are returned as a vector with the probability that the image\n",
    "# contains the class represented by that index.\n",
    "model.predict(input, predictions)\n",
    "\n",
    "# Let's grab the value of the top prediction and its index, which represents the top most \n",
    "# confident match and the class or category it belongs to.\n",
    "topN = helpers.get_top_n_predictions(predictions, 1)\n",
    "\n",
    "# See whether the prediction is in one of our groups\n",
    "group = \"\"\n",
    "caption = \"\"\n",
    "label = \"\"\n",
    "if len(topN) > 0:\n",
    "    top = topN[0]\n",
    "    label = categories[top[0]]\n",
    "    if label_in_set(label, dogs):\n",
    "        group = \"Dog\"\n",
    "    elif label_in_set(label, cats):\n",
    "        group = \"Cat\"\n",
    "\n",
    "if group != \"\":\n",
    "    # A group was detected, so take action\n",
    "    top = topN[0]\n",
    "    take_action(group)\n",
    "    headerText = \"(\" + str(int(top[1]*100)) + \"%) \" + group\n",
    "    lastPredictionTime = now\n",
    "    lastHist = hist\n",
    "else:\n",
    "    # No group was detected\n",
    "    headerText = \"? \" + label\n",
    "\n",
    "print('PREDICTION')\n",
    "print(headerText, flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
